<!-- # --- -->
<!-- # title: "Rafael Monteiro's Website" -->
<!-- # --- -->
#
#

 I will post here  some stochastic simulations I have been running, with an ultimate goal of sharing some ideas and codes on Simulated Annealing. A few remarks before we start:

- I don't claim originality for the results posted here: these are mostly some quick experiments I did while doing research. 
- I jump back and forth between  programming languages and softwares: some codes are in python, some are in R, some are in Matlab and, whenever symbolic computation is necessary, I use Sage. Even if you don't know any of the sintax used in one of these languages, it is very simple to read these codes and start coding your own things :)  For ML I will mostly use Python, which is one of the best languages for that purpose.

## Some notes and codes

- [An example of the rejection method](Rejection_method.html) ([pdf version](Rejection_method.pdf))
- [A quick example of simulated annealing](Simulated annealing/Simulated annealing.html) ([pdf version](Simulated annealing/Simulated annealing.pdf)) 
- [A quick comparison between the previous result using simulated annealing and gradient descent]() ([pdf version](Gradient Descent/gradient_descent.pdf)) 

For the following, it is helpful if, besides python,  you know a little bit of tensorflow (the code is "almost self explanatory", but it is always helpful if you have been previously exposed to tensorflow's idea)

- [Weight evolution and mass shuffling in a shallow NN](Weight_evolution_shallow_NN/Weight_evolution_in_shallow_NN.html) ([pdf version](Weight_evolution_shallow_NN/Weight_evolution_in_shallow_NN.pdf))
([jupyter-notebook version](Weight_evolution_shallow_NN/Weight_evolution_in_shallow_NN.ipyb))

The next notebook is a continuation of the previous one. Instead of using classical backpropagation, we use a probabilistic way to choose new weights

- [Weight evolution and mass shuffling in a shallow NN: a random walk approach](Weight_evolution_in_shallow_NN-random_walking/Weight_evolution_in_shallow_NN-random_walking.html) ([pdf version](Weight_evolution_in_shallow_NN-random_walking/Weight_evolution_in_shallow_NN-random_walking.pdf))
([jupyter-notebook version](Weight_evolution_in_shallow_NN-random_walking/Weight_evolution_in_shallow_NN-random_walking.ipynb))

Inthe next notebook is short we run a  [Convolution Neural Network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network) in order to do predictions in a supervised learning problem. Later on, through dimensional reduction (which will be explained in the notebook), we run a Neural Network prediction model in a lower dimensional manifold which, due to its "high quality", ends up providing good information for a prediction algorithm as good or even more efficient than CNN.

For this program we use [__Keras__](https://keras.io) and (very little) Tensorflow.

- [Convolution Neural Networks, dimensional reduction, and comparison with NNs](CNNs_NNs/Comparing_CNN_and_NN_in_reduced_dimension_model.html) ([pdf version](CNNs_NNs/Comparing_CNN_and_NN_in_reduced_dimension_model.pdf))
([jupyter-notebook version](CNNs_NNs/Comparing_CNN_and_NN_in_reduced_dimension_model.ipynb))

The following note concerns LASSO, Ridge regression, and Least squares regression. It can be also be seen, and that's the perspective that I adopt in the notes, as a 1 layer NN where one forgets to use an activation function. 
There is also an interesting issue regarding to symmetry, labeling, and penalization. For this study we only use standard libraries (sklearn and numpy).

- [Ridge regression, Least squares, and Lasso](Lasso_RidReg_LeasSq/Lasso_Least_squares_and_Ridge_regression.html) ([pdf version](Lasso_RidReg_LeasSq/Lasso_Least_squares_and_Ridge_regression.pdf))
([jupyter-notebook version](Lasso_RidReg_LeasSq/Lasso_Least_squares_and_Ridge_regression.ipynb))


I recently read an interesting (old) paper by Daniel Hillis, [Co-evolving parasites improve simulated evolution as an optimization procedure](https://www.sciencedirect.com/science/article/pii/0167278990900762), on the idea of co-evolving parasites applied on an optimization problem. I decided to play a bit with it.  This is somehow related to the above post on weight evolution and mass shuffling, but the heuristics for parameter search is way more interesting than the one I had designed therein.
Nevertheless, they parallel in the sense that both are stochastic algorithms,  and  "Backpropagation-free".


- [Training an Artificial Neural Network using a Genetic Algorithm](Genetic_Algorithms/ga_ANN.html) ([pdf version](Genetic_Algorithms/ga_ANN.pdf))
([jupyter-notebook version](Genetic_Algorithms/ga_ANN.ipynb))([markdown-version](Genetic_Algorithms/ga_ANN.md))

